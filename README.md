![mobile](https://user-images.githubusercontent.com/88651007/147491124-c654916a-558a-49ce-ab4c-1da266c879c4.jpg)
<h1 align="center"><b> Mobile-Price-Range-Prediction-Analysis </b></h1>
<h3 align="center"> AlmaBetter Verfied Project - <a href="https://www.almabetter.com/"> AlmaBetter School </a> </h5>
<h2> :floppy_disk: Project Files Description</h2>
<p>The dataset contains 21 features (20 independent and 1 dependent), price column (dependent) has multiclass categorical value (0,1,2,3) in which the price of a mobile has to be categorized and independent columns include battery_power, mobile_weight, ram, talk_time, dual_sim etc.</p>

![-----------------------------------------------------](https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/rainbow.png)

<h2> :book: Logistic Regression</h2>
<p>Logistic Regression utilizes the power of regression to do classification and has been doing so exceedingly well for several decades now, to remain amongst the most popular models. One of the main reasons for the model’s success is its power of explainability i.e. calling-out the contribution of individual predictors, quantitatively.</p>
<p>Unlike regression which uses Least Squares, the model uses Maximum Likelihood to fit a sigmoid-curve on the target variable distribution.</p>
<p>Given the model’s susceptibility to multi-collinearity, applying it step-wise turns out to be a better approach in finalizing the chosen predictors of the model.</p>
<p>The algorithm is a popular choice in many natural language processing tasks e.g. toxic speech detection, topic classification, etc.</p>
<p align="center" width="100%">
    <img width="40%" src="https://user-images.githubusercontent.com/88651007/147535712-546a2575-127c-441c-a165-21fb93f61f63.png">
</p>
<h2> :book: KNN</h2>
<p>K-Nearest Neighbor (KNN) algorithm predicts based on the specified number (k) of the nearest neighboring data points. Here, the pre-processing of the data is significant as it impacts the distance measurements directly. Unlike others, the model does not have a mathematical formula, neither any descriptive ability.</p>
<p>Here, the parameter ‘k’ needs to be chosen wisely; as a value lower than optimal leads to bias, whereas a higher value impacts prediction accuracy.</p>
<p>It is a simple, fairly accurate model preferable mostly for smaller datasets, owing to huge computations involved on the continuous predictors.</p>
<p>At a simple level, KNN may be used in a bivariate predictor setting e.g. height and weight, to determine the gender given a sample.</p>
<p align="center" width="100%">
    <img width="40%" src="https://user-images.githubusercontent.com/88651007/147538321-628e15e1-8edf-4854-bb3a-c035f49fa38b.png"></p>
<h2> :book: Decision Tree</h2>
<p>Decision tree is the most powerful and popular tool for classification and prediction. A Decision tree is a flowchart like tree structure, where each internal node denotes a test on an attribute, each branch represents an outcome of the test, and each leaf node (terminal node) holds a class label.</p>
<p>Decision trees classify instances by sorting them down the tree from the root to some leaf node, which provides the classification of the instance. An instance is classified by starting at the root node of the tree, testing the attribute specified by this node, then moving down the tree branch corresponding to the value of the attribute as shown in the above figure. This process is then repeated for the subtree rooted at the new node.</p>
<p align="center" width="100%">
    <img width="40%" src="https://user-images.githubusercontent.com/88651007/147539491-5997c705-9e3d-465e-b5d6-6a20576684aa.png"></p>

![M1](https://user-images.githubusercontent.com/88651007/147546017-7e3c4e12-2108-45e5-a6af-d28580c5bda2.PNG)

![M2](https://user-images.githubusercontent.com/88651007/147546032-31ee26ec-0125-4387-85d4-327277094006.PNG)

![M3](https://user-images.githubusercontent.com/88651007/147546052-cacfaba9-ca65-4ce4-85d8-c33286b14366.PNG)

![M4](https://user-images.githubusercontent.com/88651007/147546069-37dfb541-391f-425b-a8db-f15312275f34.PNG)

![M5](https://user-images.githubusercontent.com/88651007/147546084-56f39e90-fd1d-41a5-b0f0-7f63f6eda1c9.PNG)

![M6](https://user-images.githubusercontent.com/88651007/147546459-e96a1259-a9d9-4f55-a53b-8ef230f05cd4.PNG)

![M7](https://user-images.githubusercontent.com/88651007/147546483-45dd191b-78b4-4f1f-a0af-29e901b876c2.PNG)


![-----------------------------------------------------](https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/rainbow.png)

<!-- CREDITS -->
<h2 id="credits"> :scroll: Credits</h2>

Sidhartha Patel | Avid Learner | Data Scientist | Machine Learning Engineer | Deep Learning enthusiast

<p> <i> Contact me for Data Science Project Collaborations</i></p>


[![LinkedIn Badge](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](www.linkedin.com/in/sidpatel96)
[![GitHub Badge](https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/SIDPATEL-9496)
[![Medium Badge](https://img.shields.io/badge/Medium-1DA1F2?style=for-the-badge&logo=medium&logoColor=white)](	https://medium.com/@sidharthap1996)
[![Resume Badge](https://img.shields.io/badge/resume-0077B5?style=for-the-badge&logo=resume&logoColor=white)](https://drive.google.com/file/d/1FzoVHxqus-tCxejkEy6VMEbcqzTVQuST/view?usp=sharing)


![-----------------------------------------------------](https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/rainbow.png)

<h2> :books: References</h2>
<ul>
  <li><p>,analyticsvidhya.com, 'Classification models'. [Online].</p>
      <p>Available: https://www.analyticsvidhya.com/blog/2020/11/popular-classification-models-for-machine-learning/</p>
  </li>
  <li><p>ibm.com, 'Logistic regression'. [Online].</p>
      <p>Available: https://www.ibm.com/in-en/topics/logistic-regression</p>
  </li>
  <li><p>Wikipedia.org, 'k-nearest neighbours'. [Online].</p>
      <p>Available: https://www.geeksforgeeks.org/k-nearest-neighbours/</p>
  </li>
  <li><p>towardsdatascience.com, 'Decision trees in machine learning'. [Online].</p>
      <p>Available: https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052</p>
  </li>
  <li><p>towardsdatascience.com, 'Basis of Ensemble methods'. [Online].</p>
      <p>Available: https://towardsdatascience.com/decision-trees-understanding-the-basis-of-ensemble-methods-e075d5bfa704</p>
  </li>
  <li><p>towardsdatascience.com, 'Ensemble methods'. [Online].</p>
      <p>Available: https://towardsdatascience.com/decision-tree-ensembles-bagging-and-boosting-266a8ba60fd9/</p>
  </li>
  <li><p>analyticsvidhya.com, 'Important model evaluation metrics'. [Online].</p>
      <p>Available: https://www.analyticsvidhya.com/blog/2019/08/11-important-model-evaluation-error-metrics/</p>
  </li>
</ul>

![-----------------------------------------------------](https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/rainbow.png)
